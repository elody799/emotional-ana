{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bfbf82-56bd-44db-a4fc-3c7a0db78d45",
   "metadata": {},
   "source": [
    "### 依赖区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbaa4720-4734-4f6f-94da-7c890ff4f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e19a2-964d-40f6-a07c-5c4d49ce59d0",
   "metadata": {},
   "source": [
    "### 1.数据清洗 ———— 删除空值、重复值、去除无意义标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bffc8f-9cec-46c1-a0f3-bc7962732675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 读取CSV文件\n",
    "df = pd.read_csv('1.csv')#需要进行情感分析的聊天记录文档\n",
    "\n",
    "# 删除具有缺失值的行\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546aa32d-2d05-44cd-a180-efa2a2938144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\86182\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.446 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       ﻿ 更博 了   爆照 了   帅 的 呀   就是 越来越 爱 你   生快 傻 缺   ...\n",
      "1         张晓鹏 jonathan   土耳其 的 事要 认真对待   哈哈   否则 直接 开除...\n",
      "2       姑娘 都 羡慕 你 呢   还有 招财猫 高兴   爱 在 蔓延   JC   哈哈   小...\n",
      "3                                               美   爱 你  \n",
      "4                            梦想 有 多 大   舞台 就 有 多 大   鼓掌  \n",
      "                              ...                        \n",
      "1124    一 公里 不到   县 医院 那个 天桥 下右 拐   米 就 到 了   谢礼 恒   我...\n",
      "1125    今天 真冷 啊   难道 又 要 穿 棉袄 了   晕   今年 的 春天 真的 是 百变 ...\n",
      "1126                                最近 几天 就 没 停止 过   伤心  \n",
      "1127                                    毒药 女流氓   怒   很惨  \n",
      "1128    呢   杰   Kelena   抓狂   搞 乜 鬼   想知   入去 GOtrip 睇...\n",
      "Name: cut, Length: 1129, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 本地 csv 文档路径\n",
    "csv_path = '1.csv'\n",
    "# 待分词的 csv 文件中的列\n",
    "document_column = 'review'\n",
    "label_column = 'label'\n",
    "pattern = u'[\\\\s\\\\d,.<>/?:;\\'\\\"[\\\\]{}()\\\\|~!\\t\"@#$%^&*\\\\-_=+，。\\n《》、？：；“”‘’｛｝【】（）…￥！—┄－]+'\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "\n",
    "# 仅保留 \"label\" 和 \"review\" 两列\n",
    "df = df[[label_column, document_column]]\n",
    "\n",
    "# 删除 review 列缺失的行\n",
    "df = df[pd.notna(df[document_column])]\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df = df.rename(columns={\n",
    "    document_column: 'text'\n",
    "})\n",
    "df = df.rename(columns={\n",
    "    label_column: 'label'\n",
    "})\n",
    "df['cut'] = df['text'].apply(lambda x: str(x))\n",
    "df['cut'] = df['cut'].apply(lambda x: re.sub(pattern, ' ', x))\n",
    "df['cut'] = df['cut'].apply(lambda x: \" \".join(jieba.lcut(x)))\n",
    "print(df['cut'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a9b7f4-16a5-42e9-ae5c-9dffcd4b9b2f",
   "metadata": {},
   "source": [
    "### 删去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b444ce-e885-4316-90a0-af2cee18c71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                            ﻿ 更博 爆照 帅 越来越 爱 生快 傻 缺 爱 爱 爱\n",
      "1        张晓鹏 jonathan 土耳其 事要 认真对待 开除 丁丁 看 世界 很 细心 酒店 都 ok\n",
      "2       姑娘 都 羡慕 招财猫 爱 蔓延 jc 小 学徒 一枚 明天 见 李欣芸 sharonle ...\n",
      "3                                                     美 爱\n",
      "4                                            梦想 大 舞台 大 鼓掌\n",
      "                              ...                        \n",
      "1124    公里 不到 县 医院 天桥 下右 拐 米 谢礼 恒 太 霸道 好 远 古倒 吃 点 真心 找...\n",
      "1125                          真冷 难道 穿 棉袄 晕 春天 真的 百变 莫测 抓狂\n",
      "1126                                           几天 没 停止 伤心\n",
      "1127                                          毒药 女流氓 怒 很惨\n",
      "1128    杰 kelena 抓狂 搞 乜 鬼 想知 入去 gotrip 睇 睇 http t cn a...\n",
      "Name: cut1, Length: 1129, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 读取停用词表\n",
    "def load_stopwords(stopwords_file):\n",
    "    stopwords = set()\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stopwords.add(line.strip())\n",
    "    return stopwords\n",
    "\n",
    "# 分词结果\n",
    "cut_words = df['cut'].tolist()\n",
    "\n",
    "# 加载停用词表\n",
    "stopwords = load_stopwords(\"baidu_stopwords.txt\")\n",
    "\n",
    "# 初始化词干提取器\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# 去除停用词并进行词干提取\n",
    "filtered_words = []\n",
    "for words in cut_words:\n",
    "    filtered_words.append([stemmer.stem(word) for word in words.split() if word not in stopwords])\n",
    "\n",
    "# 将去除停用词并进行词干提取后的结果更新到DataFrame中\n",
    "df['filtered_cut_stemmed'] = filtered_words\n",
    "\n",
    "df['cut1'] = [' '.join(words) for words in df['filtered_cut_stemmed']]\n",
    "\n",
    "# 输出结果\n",
    "print(df['cut1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd641a2f-6195-4540-9866-3b86ed034a6e",
   "metadata": {},
   "source": [
    "### 词性标注（中文）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5813c49a-7056-49f1-9eb1-42a1b71b178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [(博, v), (爆照, v), (帅, nr), (爱, v), (生快, v), (傻...\n",
      "1       [(张晓鹏, nr), (土耳其, ns), (事, n), (要, v), (开除, v)...\n",
      "2       [(姑娘, n), (羡慕, v), (招财猫, nr), (爱, v), (蔓延, v),...\n",
      "3                                       [(美, ns), (爱, v)]\n",
      "4             [(梦想, n), (大, a), (舞台, n), (大, a), (鼓掌, v)]\n",
      "                              ...                        \n",
      "1124    [(不到, v), (医院, n), (天桥, ns), (谢礼, nr), (恒, nr)...\n",
      "1125    [(真冷, a), (棉袄, n), (晕, v), (百变, nz), (莫测, nr),...\n",
      "1126                           [(没, v), (停止, v), (伤心, n)]\n",
      "1127                [(毒药, n), (女流氓, n), (怒, vg), (很惨, a)]\n",
      "1128    [(杰, nr), (抓狂, v), (搞, v), (乜, nr), (鬼, n), (想...\n",
      "Name: cut2, Length: 1129, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#词性标注（中文）\n",
    "def pos_tagging(text):\n",
    "    words = pseg.cut(text)\n",
    "    tagged_words = [(word.word, word.flag) for word in words if word.flag.startswith('n') or word.flag.startswith('v') or word.flag.startswith('a')]\n",
    "    return tagged_words\n",
    "\n",
    "df['cut2'] = df['cut1'].apply(pos_tagging)\n",
    "print(df['cut2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe0d7cc-946a-4d75-b777-59070e6ac5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   博 爆照 帅 爱 生快 傻 缺 爱 爱 爱\n",
      "1                            张晓鹏 土耳其 事 要 开除 丁丁 看 世界 细心 酒店\n",
      "2                     姑娘 羡慕 招财猫 爱 蔓延 小 学徒 见 李欣芸 大佬 范儿 书呆子\n",
      "3                                                     美 爱\n",
      "4                                            梦想 大 舞台 大 鼓掌\n",
      "                              ...                        \n",
      "1124    不到 医院 天桥 谢礼 恒 霸道 好 远 古 倒 吃 找 吃 泪 敏 嘴 记到 去 吃 吃 ...\n",
      "1125                                     真冷 棉袄 晕 百变 莫测 抓狂\n",
      "1126                                              没 停止 伤心\n",
      "1127                                          毒药 女流氓 怒 很惨\n",
      "1128                                 杰 抓狂 搞 乜 鬼 想知 入去 睇 睇\n",
      "Name: cut2, Length: 1129, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#在词性标注之后删去非中文部分、括号、每行开头和末尾的空格\n",
    "def clean_tagged_words(tagged_words): \n",
    "    cleaned_words = [re.sub(r'[^[\\u4e00-\\u9fa5 ]]','', word[0]).strip() \n",
    "    for word in tagged_words] \n",
    "    return cleaned_words\n",
    "\n",
    "df['cut2'] = df['cut2'].apply(clean_tagged_words) \n",
    "df['cut2'] = df['cut2'].apply(lambda x: \" \".join(x))\n",
    "print(df['cut2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfd098-bb2b-4095-8766-2e27b14442b7",
   "metadata": {},
   "source": [
    "### 对数据集进行观察 构造TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892e63db-e92b-4c03-8ee9-c39c5d700227",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(df['cut2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a7ccb87-0074-4754-9a91-8eb0d4425ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b40be35-7ab2-483c-99b9-28901c238fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征词列表\n",
    "feature_names = tf_idf_vectorizer.get_feature_names_out()\n",
    "# 特征词 TF-IDF 矩阵\n",
    "tfidf_matrix = tf_idf.toarray()\n",
    "feature_names_df = pd.DataFrame(tfidf_matrix,columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df4b3c0-87aa-490d-8954-b3da3bd1a52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['一体', '一厂', '一品堂', ..., '龙行', '龙门', '龟苓膏'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b5ec431-7024-4f95-a8a7-cd5e0b8d1c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一体</th>\n",
       "      <th>一厂</th>\n",
       "      <th>一品堂</th>\n",
       "      <th>一块钱</th>\n",
       "      <th>一大</th>\n",
       "      <th>一家人</th>\n",
       "      <th>一景</th>\n",
       "      <th>一楼</th>\n",
       "      <th>一流</th>\n",
       "      <th>一盘菜</th>\n",
       "      <th>...</th>\n",
       "      <th>龙之梦</th>\n",
       "      <th>龙儿</th>\n",
       "      <th>龙妈</th>\n",
       "      <th>龙洋</th>\n",
       "      <th>龙湖</th>\n",
       "      <th>龙猫</th>\n",
       "      <th>龙珠果</th>\n",
       "      <th>龙行</th>\n",
       "      <th>龙门</th>\n",
       "      <th>龟苓膏</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1129 rows × 5783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       一体   一厂  一品堂  一块钱   一大  一家人   一景   一楼   一流  一盘菜  ...  龙之梦   龙儿   龙妈  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1124  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1125  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1126  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1127  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1128  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "       龙洋   龙湖   龙猫  龙珠果   龙行   龙门  龟苓膏  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "1124  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1125  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1126  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1127  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1128  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1129 rows x 5783 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720fbbd-495c-4eb3-8fb7-6b7a2ffe7095",
   "metadata": {},
   "source": [
    "### 2.词向量word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a4e8b5-e698-47be-ad67-dab3d70d238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text processing 将每个文本按照分词的预处理步骤进行处理，例如去除标点符号、转换为小写等。\n",
    "texts = [simple_preprocess(text) for text in df['cut2']]\n",
    "\n",
    "# Create a dictionary 根据处理后的文本集合texts创建词典，将每个单词映射为唯一的整数ID。\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Create a bag-of-words corpus 将每个文本转换为词袋向量表示，即将文本中的每个单词映射为其在词典中的整数ID，并计算每个单词出现的频次。\n",
    "corpus_vec = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553215ab-8ae2-48ac-9c62-e859dcdbfb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 训练 Word2Vec 模型\n",
    "model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a97e1a-f750-4610-b62a-1ffede24b2bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd413fa4-9b25-436f-a8d8-f5e9aeeaaf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86182\\AppData\\Local\\Temp\\ipykernel_24544\\3597145144.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  X[i] /= len(text)  # Normalize the vector by the number of words\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a matrix to store the word embeddings\n",
    "vector_size = model.vector_size\n",
    "X = np.zeros((len(texts), vector_size))\n",
    "\n",
    "# Convert each text into a vector representation using Word2Vec\n",
    "for i, text in enumerate(texts):\n",
    "    for word in text:\n",
    "        if word in model.wv.key_to_index:\n",
    "            X[i] += model.wv.get_vector(word)\n",
    "    X[i] /= len(text)  # Normalize the vector by the number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a055c05-e9a4-4dd3-ba3e-2a82e865f19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.82401690e-03, -2.08525515e-03,  2.72756489e-03,  5.61258119e-03,\n",
       "       -5.32728472e-03, -4.44843744e-04,  1.26467148e-03,  6.89877430e-04,\n",
       "        1.79825863e-03, -3.59690360e-04,  6.56268753e-03,  1.28225720e-03,\n",
       "        3.11560836e-03,  4.37039253e-03,  4.75187873e-04, -5.21656382e-03,\n",
       "        7.31776779e-04,  5.31359261e-03, -7.88579400e-03, -8.38938340e-03,\n",
       "        2.50353167e-04,  1.67803237e-03,  3.18458630e-03,  1.08465680e-03,\n",
       "        4.74671771e-03, -1.91148673e-03,  2.33494047e-03,  7.85078900e-03,\n",
       "       -9.06837452e-03, -1.89110404e-03, -2.46525463e-03, -1.90839928e-03,\n",
       "       -1.79513590e-03, -1.31911769e-03,  2.01542528e-03,  3.23949529e-03,\n",
       "        7.05358076e-03, -4.44977637e-03, -1.96400052e-03, -2.14825672e-03,\n",
       "       -6.03725641e-03,  1.09424489e-03, -2.02334969e-03, -2.29279418e-03,\n",
       "       -2.91347319e-03,  1.42337578e-03, -3.99112422e-03, -1.15794347e-03,\n",
       "        6.59697542e-03,  7.31098497e-03, -3.06219328e-03,  9.69508042e-04,\n",
       "        1.36841360e-03,  1.99575525e-03,  5.60044132e-03, -9.30778993e-04,\n",
       "       -4.57246865e-04, -4.00524156e-03, -4.85295911e-03,  2.95652387e-04,\n",
       "        1.52694341e-03,  4.67489877e-03,  4.39671344e-04, -5.37828884e-03,\n",
       "       -6.14984644e-03,  1.89560652e-03,  6.26309969e-05,  1.92059066e-03,\n",
       "       -6.52543067e-03,  2.09126441e-03,  1.76362492e-03,  5.28158310e-03,\n",
       "       -1.75281152e-03, -3.89512357e-04,  9.80878530e-04,  3.10588220e-03,\n",
       "        1.96220865e-03,  9.77621724e-05,  1.98437910e-04, -5.76054328e-03,\n",
       "        3.02021070e-03, -2.74023573e-03,  3.50537984e-03,  6.07859284e-03,\n",
       "       -7.11821020e-03,  3.29201544e-03,  3.13779997e-03,  2.37867197e-04,\n",
       "        9.85688569e-04,  4.46750247e-03,  2.52194690e-03, -2.70449593e-03,\n",
       "        9.43714908e-04,  1.99353568e-03,  3.25838958e-03,  2.14028405e-03,\n",
       "        6.22776958e-05, -1.40536235e-03, -1.98091341e-04,  3.51729857e-03])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cc277ae-4cec-4672-9f7d-82c57c6ff706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['博 爆照 帅 爱 生快 傻 缺 爱 爱 爱' '张晓鹏 土耳其 事 要 开除 丁丁 看 世界 细心 酒店'\n",
      " '姑娘 羡慕 招财猫 爱 蔓延 小 学徒 见 李欣芸 大佬 范儿 书呆子' ... '没 停止 伤心' '毒药 女流氓 怒 很惨'\n",
      " '杰 抓狂 搞 乜 鬼 想知 入去 睇 睇']\n",
      "Y: [1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary libraries and packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Update features (X) and labels (y) accordingly\n",
    "X = df['cut2'].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(\"X:\", X)\n",
    "print(\"Y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3dfa10-ef07-4987-a203-c5cebf97f670",
   "metadata": {},
   "source": [
    "### 训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7642d2fb-2fad-4145-9d3a-3aa72859d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from gensim import corpora\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "vocab_dim = 100\n",
    "n_iterations = 1\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "maxlen = 100\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data(w2indx, w2vec, data, y):\n",
    "    n_symbols = len(w2indx) + 1  # 补上索引为0（频数小于10）的词\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))  \n",
    "    for word, index in w2indx.items(): \n",
    "        embedding_weights[index, :] = w2vec[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.2)\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)  # 转换为one-hot特征\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "    return n_symbols, embedding_weights, x_train, y_train, x_test, y_test\n",
    "\n",
    "def word2vec_train(data):\n",
    "    \"\"\"Train Word2Vec model\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Segmented 2D list\n",
    "    \"\"\"\n",
    "    model = Word2Vec(vector_size=vocab_dim, min_count=n_exposures, window=window_size, workers=cpu_count, epochs=n_iterations)\n",
    "    #model = Word2Vec(size=vocab_dim, min_count=n_exposures, window=window_size, workers=cpu_count, iter=n_iterations)\n",
    "    model.build_vocab(data)\n",
    "    # model.train(data, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save('Word2vec_model.pkl')\n",
    "\n",
    "\n",
    "def create_dictionaries(model=None, data=None):\n",
    "\n",
    "    if (data is not None) and (model is not None):\n",
    "        w2indx = {word: index + 1 for index, word in enumerate(model.wv.index_to_key)}\n",
    "        f = open(\"word2index.txt\", 'w', encoding='utf8')\n",
    "        for key in w2indx:\n",
    "            f.write(str(key))\n",
    "            f.write(' ')\n",
    "            f.write(str(w2indx[key]))\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "        w2vec = {word: model.wv.get_vector(word) for word in w2indx}\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            data = []\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data  # word => index\n",
    "\n",
    "        data = parse_dataset(data)\n",
    "        data = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "        return w2indx, w2vec, data\n",
    "    else:\n",
    "        print('Text is empty!')  \n",
    "\n",
    "\n",
    "# Combine the segmented lists\n",
    "X = df['cut2'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_train(X)\n",
    "\n",
    "model = Word2Vec.load('Word2vec_model.pkl')\n",
    "\n",
    "w2indx, w2vec, data = create_dictionaries(model=model, data=X)\n",
    "\n",
    "# Get the data for training and testing\n",
    "n_symbols, embedding_weights, x_train, y_train, x_test, y_test = get_data(w2indx, w2vec, data, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ac08b-2822-43f0-9aa3-aa302703f326",
   "metadata": {},
   "source": [
    "### 3.  七种模型的预测准确率（Naive Bayes、LR、SVM、KNN、DT、RF、LSTM）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af6f7c4e-306d-4478-acc0-8e630131cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.5796460176991151\n",
      "Logistic Regression Accuracy: 0.5619469026548672\n",
      "Support Vector Machine Accuracy: 0.5973451327433629\n",
      "K-Nearest Neighbors Accuracy: 0.588495575221239\n",
      "Decision Tree Accuracy: 0.6415929203539823\n",
      "Random Forest Accuracy: 0.6283185840707964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def train_models(x_train, y_train, x_test, y_test):\n",
    "    # Naive Bayes\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_train, y_train)\n",
    "    nb_pred = nb.predict(x_test)\n",
    "    nb_pred_flat = nb_pred.flatten()\n",
    "    nb_pred_labels = np.where(nb_pred > 0.5, 1, 0)  \n",
    "    nb_test_labels = np.argmax(y_test, axis=1)\n",
    "    accuracy_nb = np.mean(nb_pred_labels == nb_test_labels)\n",
    "    print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    lr_pred = lr.predict(x_test)\n",
    "    lr_pred_flat = lr_pred.flatten()\n",
    "    lr_pred_labels = np.where(lr_pred > 0.5, 1, 0)  \n",
    "    lr_test_labels = np.argmax(y_test, axis=1)\n",
    "    accuracy_lr = np.mean(lr_pred_labels == lr_test_labels)\n",
    "    print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "\n",
    "    # Support Vector Machine\n",
    "    svm = SVC()\n",
    "    svm.fit(x_train, y_train)\n",
    "    svm_pred = svm.predict(x_test)\n",
    "    svm_pred_flat = svm_pred.flatten()\n",
    "    svm_pred_labels = np.where(svm_pred > 0.5, 1, 0)  \n",
    "    svm_test_labels = np.argmax(y_test, axis=1)\n",
    "    accuracy_svm = np.mean(svm_pred_labels == svm_test_labels)\n",
    "    print(\"Support Vector Machine Accuracy:\", accuracy_svm)\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(x_train, y_train)\n",
    "    knn_pred = knn.predict(x_test)\n",
    "    knn_pred_flat = svm_pred.flatten()\n",
    "    knn_pred_labels = np.where(knn_pred > 0.5, 1, 0)  \n",
    "    knn_test_labels = np.argmax(y_test, axis=1)\n",
    "    accuracy_knn = np.mean(knn_pred_labels == knn_test_labels)\n",
    "    print(\"K-Nearest Neighbors Accuracy:\", accuracy_knn)\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(x_train, y_train)\n",
    "    dt_pred = dt.predict(x_test)\n",
    "    dt_pred_flat = dt_pred.flatten()\n",
    "    dt_pred_labels = np.where(dt_pred > 0.5, 1, 0)  \n",
    "    dt_test_labels = np.argmax(y_test, axis=1)\n",
    "    accuracy_dt = np.mean(dt_pred_labels == dt_test_labels)\n",
    "    print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=10)\n",
    "    rf.fit(x_train, y_train)\n",
    "    rf_pred = rf.predict(x_test)\n",
    "    rf_pred_flat = rf_pred.flatten()\n",
    "    rf_pred_labels = np.where(rf_pred > 0.5, 1, 0)  \n",
    "    rf_test_labels = np.argmax(y_test, axis=1)\n",
    "    accuracy_rf = np.mean(rf_pred_labels == rf_test_labels)\n",
    "    print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "\n",
    "    return\n",
    "\n",
    "# Train and evaluate different models\n",
    "train_models(x_train, y_train, x_test, y_test)\n",
    "\n",
    "#总体来说预测结果较好的是：SVM、DT、RF。仍旧需要进一步比较，用K折交叉验证！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fd038-5eb5-44ec-9d80-21e5c76ae62b",
   "metadata": {},
   "source": [
    "### 使用K折交叉验证求取模型的平均预测准确率（朴素贝叶斯、LR、SVM、KNN、DT、RF、LSTM）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69fc9765-75a1-42ee-9135-61c8d67045a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.4945054945054945\n",
      "Naive Bayes Accuracy: 0.5714285714285714\n",
      "Naive Bayes Accuracy: 0.5054945054945055\n",
      "Naive Bayes Accuracy: 0.6222222222222222\n",
      "Naive Bayes Accuracy: 0.5\n",
      "Naive Bayes Accuracy: 0.4888888888888889\n",
      "Naive Bayes Accuracy: 0.5111111111111111\n",
      "Naive Bayes Accuracy: 0.6444444444444445\n",
      "Naive Bayes Accuracy: 0.6\n",
      "Naive Bayes Accuracy: 0.5444444444444444\n",
      "Average Naive Bayes Accuracy: 0.5482539682539682\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "def train_models_NaiveBayes(x, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        nb = MultinomialNB()\n",
    "        nb.fit(x_train, y_train)\n",
    "        nb_pred = nb.predict(x_test)\n",
    "\n",
    "        nb_pred_labels = np.where(nb_pred > 0.5, 1, 0)\n",
    "        accuracy_nb = np.mean(nb_pred_labels == y_test)\n",
    "        accuracies.append(accuracy_nb)\n",
    "        print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(\"Average Naive Bayes Accuracy:\", average_accuracy)\n",
    "\n",
    "# Assuming x_train, y_train, x_test, y_test are already defined\n",
    "train_models_NaiveBayes(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4b6a5a2-86fd-4fef-9fe3-3eb8e55ae861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.4835164835164835\n",
      "Logistic Regression Accuracy: 0.5494505494505495\n",
      "Logistic Regression Accuracy: 0.5164835164835165\n",
      "Logistic Regression Accuracy: 0.6111111111111112\n",
      "Logistic Regression Accuracy: 0.5444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.5\n",
      "Logistic Regression Accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6333333333333333\n",
      "Logistic Regression Accuracy: 0.6222222222222222\n",
      "Logistic Regression Accuracy: 0.5555555555555556\n",
      "Average Logistic Regression Accuracy: 0.5616117216117216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86182\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def train_models_LR(x, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(x_train, y_train)\n",
    "        lr_pred = lr.predict(x_test)\n",
    "        lr_pred_labels = np.where(lr_pred > 0.5, 1, 0)  \n",
    "        accuracy_lr = np.mean(lr_pred_labels == y_test)\n",
    "        accuracies.append(accuracy_lr)\n",
    "        print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(\"Average Logistic Regression Accuracy:\", average_accuracy)\n",
    "\n",
    "# Assuming x_train, y_train, x_test, y_test are already defined\n",
    "train_models_LR(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59ec0b55-12c1-402e-9182-0157394b7970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Accuracy: 0.6043956043956044\n",
      "Support Vector Machine Accuracy: 0.5274725274725275\n",
      "Support Vector Machine Accuracy: 0.5934065934065934\n",
      "Support Vector Machine Accuracy: 0.6222222222222222\n",
      "Support Vector Machine Accuracy: 0.5666666666666667\n",
      "Support Vector Machine Accuracy: 0.5444444444444444\n",
      "Support Vector Machine Accuracy: 0.6666666666666666\n",
      "Support Vector Machine Accuracy: 0.6222222222222222\n",
      "Support Vector Machine Accuracy: 0.5\n",
      "Support Vector Machine Accuracy: 0.5111111111111111\n",
      "Average Support Vector Machine Accuracy: 0.5758608058608059\n"
     ]
    }
   ],
   "source": [
    "def train_models_SVM(x, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        svm = SVC()\n",
    "        svm.fit(x_train, y_train)\n",
    "        svm_pred = svm.predict(x_test)\n",
    "        svm_pred_labels = np.where(svm_pred > 0.5, 1, 0)  \n",
    "        accuracy_svm = np.mean(svm_pred_labels == y_test)\n",
    "        accuracies.append(accuracy_svm)\n",
    "        print(\"Support Vector Machine Accuracy:\", accuracy_svm)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(\"Average Support Vector Machine Accuracy:\", average_accuracy)\n",
    "\n",
    "# Assuming x_train, y_train, x_test, y_test are already defined\n",
    "train_models_SVM(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55bf7b5e-131e-48a9-998f-8a29e688935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Accuracy: 0.5604395604395604\n",
      "K-Nearest Neighbors Accuracy: 0.4175824175824176\n",
      "K-Nearest Neighbors Accuracy: 0.6153846153846154\n",
      "K-Nearest Neighbors Accuracy: 0.5888888888888889\n",
      "K-Nearest Neighbors Accuracy: 0.4777777777777778\n",
      "K-Nearest Neighbors Accuracy: 0.5666666666666667\n",
      "K-Nearest Neighbors Accuracy: 0.5111111111111111\n",
      "K-Nearest Neighbors Accuracy: 0.5777777777777777\n",
      "K-Nearest Neighbors Accuracy: 0.4777777777777778\n",
      "K-Nearest Neighbors Accuracy: 0.5111111111111111\n",
      "Average K-Nearest Neighbors Accuracy: 0.5304517704517705\n"
     ]
    }
   ],
   "source": [
    "def train_models_KNN(x, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        knn = KNeighborsClassifier(n_neighbors=3)\n",
    "        knn.fit(x_train, y_train)\n",
    "        knn_pred = knn.predict(x_test)\n",
    "        knn_pred_labels = np.where(knn_pred > 0.5, 1, 0)  \n",
    "        accuracy_knn = np.mean(knn_pred_labels == y_test)\n",
    "        accuracies.append(accuracy_knn)\n",
    "        print(\"K-Nearest Neighbors Accuracy:\", accuracy_knn)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(\"Average K-Nearest Neighbors Accuracy:\", average_accuracy)\n",
    "\n",
    "# Assuming x_train, y_train, x_test, y_test are already defined\n",
    "train_models_KNN(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e99bc77b-890e-4ca3-b419-a8792771208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.6813186813186813\n",
      "Decision Tree Accuracy: 0.6263736263736264\n",
      "Decision Tree Accuracy: 0.6373626373626373\n",
      "Decision Tree Accuracy: 0.7\n",
      "Decision Tree Accuracy: 0.6555555555555556\n",
      "Decision Tree Accuracy: 0.5777777777777777\n",
      "Decision Tree Accuracy: 0.6555555555555556\n",
      "Decision Tree Accuracy: 0.7222222222222222\n",
      "Decision Tree Accuracy: 0.7111111111111111\n",
      "Decision Tree Accuracy: 0.6222222222222222\n",
      "Average Decision Tree Accuracy: 0.6589499389499389\n"
     ]
    }
   ],
   "source": [
    "def train_models_DT(x, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt.fit(x_train, y_train)\n",
    "        dt_pred = dt.predict(x_test)\n",
    "        dt_pred_labels = np.where(dt_pred > 0.5, 1, 0)  \n",
    "        accuracy_dt = np.mean(dt_pred_labels == y_test)\n",
    "        accuracies.append(accuracy_dt)\n",
    "        print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(\"Average Decision Tree Accuracy:\", average_accuracy)\n",
    "\n",
    "# Assuming x_train, y_train, x_test, y_test are already defined\n",
    "train_models_DT(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a156cd11-e401-425a-9ff3-145c6de9dfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7032967032967034\n",
      "Random Forest Accuracy: 0.5274725274725275\n",
      "Random Forest Accuracy: 0.6483516483516484\n",
      "Random Forest Accuracy: 0.6444444444444445\n",
      "Random Forest Accuracy: 0.6333333333333333\n",
      "Random Forest Accuracy: 0.6111111111111112\n",
      "Random Forest Accuracy: 0.6333333333333333\n",
      "Random Forest Accuracy: 0.6777777777777778\n",
      "Random Forest Accuracy: 0.5666666666666667\n",
      "Random Forest Accuracy: 0.5666666666666667\n",
      "Average Random Forest Accuracy: 0.6212454212454211\n"
     ]
    }
   ],
   "source": [
    "def train_models_RF(x, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=10)\n",
    "        rf.fit(x_train, y_train)\n",
    "        rf_pred = rf.predict(x_test)\n",
    "        rf_pred_labels = np.where(rf_pred > 0.5, 1, 0)  \n",
    "        accuracy_rf = np.mean(rf_pred_labels == y_test)\n",
    "        accuracies.append(accuracy_rf)\n",
    "        print(\"Random Forest Accuracy:\", accuracy_rf) \n",
    "               \n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(\"Average Random Forest Accuracy:\", average_accuracy)\n",
    "\n",
    "# Assuming x_train, y_train, x_test, y_test are already defined\n",
    "train_models_RF(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55bdd379-30e3-4d8e-bbb5-c41cb2e7d283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Accuracy: 0.692307710647583\n",
      "LSTM Accuracy: 0.6813187003135681\n",
      "LSTM Accuracy: 0.7142857313156128\n",
      "WARNING:tensorflow:5 out of the last 25 calls to <function Model.make_test_function.<locals>.test_function at 0x000002033993C0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "LSTM Accuracy: 0.7111111283302307\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000203403794C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "LSTM Accuracy: 0.6111111044883728\n",
      "LSTM Accuracy: 0.6000000238418579\n",
      "LSTM Accuracy: 0.6666666865348816\n",
      "LSTM Accuracy: 0.7555555701255798\n",
      "LSTM Accuracy: 0.6111111044883728\n",
      "LSTM Accuracy: 0.6777777671813965\n",
      "Average LSTM Accuracy: 0.6721245527267456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_models_LSTM(x, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # 构建 LSTM 模型\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=64, input_shape=(x_train.shape[1], 1)))  # 输入数据的形状需要适应您的数据\n",
    "        model.add(Dense(units=1, activation='sigmoid'))  # 二进制分类问题的输出层\n",
    "\n",
    "        # 编译模型\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)  # 根据需要更改参数\n",
    "\n",
    "        # 在测试数据上评估模型\n",
    "        _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"LSTM Accuracy:\", accuracy)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(\"Average LSTM Accuracy:\", average_accuracy)\n",
    "\n",
    "train_models_LSTM(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d7949-2ad8-404e-a2a0-39b363ea439a",
   "metadata": {},
   "source": [
    "### 补充 DT算法参数调优前的训练效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14d1cedf-5a25-4f33-b600-caea9344cdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9027093596059114\n",
      "Decision Tree Accuracy: 0.9310344827586207\n",
      "Decision Tree Accuracy: 0.8004926108374384\n",
      "Decision Tree Accuracy: 0.8031980319803198\n",
      "Decision Tree Accuracy: 0.7995079950799509\n",
      "Decision Tree Accuracy: 0.7847478474784748\n",
      "Decision Tree Accuracy: 0.8068880688806888\n",
      "Decision Tree Accuracy: 0.9384993849938499\n",
      "Decision Tree Accuracy: 0.931119311193112\n",
      "Decision Tree Accuracy: 0.8031980319803198\n",
      "Average Decision Tree Accuracy: 0.8501395124788687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_model_DT_with_grid_search(x, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    accuracies = []\n",
    "\n",
    "    params = {\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        dt = DecisionTreeClassifier()\n",
    "        grid_search = GridSearchCV(estimator=dt, param_grid=params, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        best_dt = grid_search.best_estimator_\n",
    "        dt_pred = best_dt.predict(x_train)\n",
    "        dt_pred_labels = np.where(dt_pred > 0.5, 1, 0)  \n",
    "        accuracy_dt = np.mean(dt_pred_labels == y_train)\n",
    "        accuracies.append(accuracy_dt)\n",
    "        print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(\"Average Decision Tree Accuracy:\", average_accuracy)\n",
    "\n",
    "# Assuming x_train, y_train, x_test, y_test are already defined\n",
    "train_model_DT_with_grid_search(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953c831-98eb-47d8-8778-2b86bc6dce13",
   "metadata": {},
   "source": [
    "### 4. 对DT算法进行改进"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00601b9-d004-4e52-aca7-5863a485956f",
   "metadata": {},
   "source": [
    "### 4.1 参数调优后的预测效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49a12b17-ef11-4aa7-9dcc-e183b31cb1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=2...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=3...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=4...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=5...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=6...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=7...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=8...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=9...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=10...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "Best Parameters: {'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 2}\n",
      "Best Accuracy: 0.7389380530973452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def parameter_search(max_depth_range, min_samples_split_range, min_samples_leaf_range, x_train, y_train, x_test, y_test):\n",
    "    best_accuracy = -np.inf\n",
    "    best_params = {}\n",
    "\n",
    "    for max_depth in max_depth_range:\n",
    "        print(f\"max_depth={max_depth}...\")\n",
    "\n",
    "        for min_samples_split in min_samples_split_range:\n",
    "            print(f\"min_samples_split={min_samples_split}...\")\n",
    "\n",
    "            for min_samples_leaf in min_samples_leaf_range:\n",
    "                print(f\"min_samples_leaf={min_samples_leaf}...\")\n",
    "\n",
    "                dt = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "                dt.fit(x_train, y_train)\n",
    "                dt_pred = dt.predict(x_test)\n",
    "                #dt_pred_labels = np.argmax(dt_pred, axis=1)\n",
    "                accuracy = np.mean(dt_pred == np.argmax(y_test, axis=1))\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params['max_depth'] = max_depth\n",
    "                    best_params['min_samples_split'] = min_samples_split\n",
    "                    best_params['min_samples_leaf'] = min_samples_leaf\n",
    "\n",
    "    return best_params, best_accuracy\n",
    "\n",
    "max_depth_range = [2,3,4,5,6,7,8,9,10]\n",
    "min_samples_split_range = [2,3,4,5,6,7,8,9,10]\n",
    "min_samples_leaf_range = [2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "best_params, best_accuracy = parameter_search(max_depth_range, min_samples_split_range, min_samples_leaf_range, x_train, y_train, x_test, y_test)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a327c5-51c1-4999-a77a-bd175947cfa9",
   "metadata": {},
   "source": [
    "### 4.2 参数调优后的训练效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b256f73c-1580-4322-840a-4619d5cb7c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=2...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=3...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=4...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=5...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=6...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=7...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=8...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=9...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "max_depth=10...\n",
      "min_samples_split=2...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=3...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=4...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=5...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=6...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=7...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=8...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=9...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "min_samples_split=10...\n",
      "min_samples_leaf=2...\n",
      "min_samples_leaf=3...\n",
      "min_samples_leaf=4...\n",
      "min_samples_leaf=5...\n",
      "min_samples_leaf=6...\n",
      "min_samples_leaf=7...\n",
      "min_samples_leaf=8...\n",
      "min_samples_leaf=9...\n",
      "min_samples_leaf=10...\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "Best Accuracy: 0.9014396456256921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def parameter_search(max_depth_range, min_samples_split_range, min_samples_leaf_range, x_train, y_train):\n",
    "    best_accuracy = -np.inf\n",
    "    best_params = {}\n",
    "\n",
    "    for max_depth in max_depth_range:\n",
    "        print(f\"max_depth={max_depth}...\")\n",
    "\n",
    "        for min_samples_split in min_samples_split_range:\n",
    "            print(f\"min_samples_split={min_samples_split}...\")\n",
    "\n",
    "            for min_samples_leaf in min_samples_leaf_range:\n",
    "                print(f\"min_samples_leaf={min_samples_leaf}...\")\n",
    "\n",
    "                dt = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "                dt.fit(x_train, y_train)\n",
    "                dt_pred = dt.predict(x_train)\n",
    "                #dt_pred_labels = np.argmax(dt_pred, axis=1)\n",
    "                accuracy = np.mean(dt_pred == y_train)\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params['max_depth'] = max_depth\n",
    "                    best_params['min_samples_split'] = min_samples_split\n",
    "                    best_params['min_samples_leaf'] = min_samples_leaf\n",
    "\n",
    "    return best_params, best_accuracy\n",
    "\n",
    "    \n",
    "max_depth_range = [2,3,4,5,6,7,8,9,10]\n",
    "min_samples_split_range = [2,3,4,5,6,7,8,9,10]\n",
    "min_samples_leaf_range = [2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "best_params, best_accuracy = parameter_search(max_depth_range, min_samples_split_range, min_samples_leaf_range, x_train, y_train)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f5206-7bce-4ccf-8cd5-036a94eaf671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136a99f-055a-4e24-9fbd-f0e5b70ac577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf52822-b435-4d64-811b-a3371e692b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03fa0f-77ee-45e5-83cc-442b9840ac0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01cd8c-b3fb-45ef-9a3a-3d1586bd0b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556d33e-6aef-4979-9e21-046db03a27b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac75d4-724f-4edc-93a7-fced7c52ee1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5d9a9-f211-4c97-9421-16ed9aa96065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a41f0-9bf9-4223-bcaf-8d3beb5a6df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e7ec4-8c22-4cbe-b1ba-3b232e785ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b11e10-a87f-4223-9996-9413f5ae35f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764af7c-2040-4e70-a506-a7c45c999ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32469ee-0d08-4b06-923d-02db39670cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a584aa-106f-4562-b646-fa08fa16664f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
